{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9b4753",
   "metadata": {},
   "source": [
    "### Importação das Bibliotecas\n",
    "Carregamento dos pacotes necessários para manipulação de dados, análise estatística, visualização e construção dos modelos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6619fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações iniciais\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Carregamento dos dados\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Inspeção das dimensões\n",
    "print(f\"Dimensões do Treino: {train_df.shape}\")\n",
    "print(f\"Dimensões do Teste: {test_df.shape}\")\n",
    "\n",
    "# Verificação inicial\n",
    "display(train_df.head())\n",
    "print(train_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434c8b8",
   "metadata": {},
   "source": [
    "### Análise da Variável Alvo e Outliers\n",
    "<p>Visualização da distribuição da variável SalePrice para verificação de normalidade e assimetria.</p>\n",
    "<p>Em seguida, análise bivariada entre área habitável (GrLivArea) e preço para identificação e remoção de outliers, conforme recomendações da documentação do dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32420c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração visual\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Visualizando a distribuição da variável alvo\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histograma com curva normal ajustada\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_df['SalePrice'], kde=True, stat=\"density\")\n",
    "(mu, sigma) = norm.fit(train_df['SalePrice'])\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "plt.plot(x, norm.pdf(x, mu, sigma), 'k', linewidth=2)\n",
    "plt.title(f'Distribuição SalePrice\\n(Skewness: {train_df[\"SalePrice\"].skew():.2f})')\n",
    "\n",
    "# Q-Q Plot para verificar normalidade\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(train_df['SalePrice'], plot=plt)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Outliers\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=train_df['GrLivArea'], y=train_df['SalePrice'])\n",
    "plt.axvline(x=4000, color='r', linestyle='--', label='Corte (> 4000 sq ft)')\n",
    "plt.title('GrLivArea vs SalePrice')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Removendo casas com mais de 4000 sq ft e preço baixo\n",
    "outliers_idx = train_df[(train_df['GrLivArea'] > 4000) & (train_df['SalePrice'] < 300000)].index\n",
    "print(f\"Removendo {len(outliers_idx)} outliers (IDs: {train_df.loc[outliers_idx, 'Id'].values})...\")\n",
    "\n",
    "train_df = train_df.drop(outliers_idx)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Novas dimensões do treino: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb1b9d",
   "metadata": {},
   "source": [
    "### Pré-processamento e Tratamento de Dados\n",
    "\n",
    "Nesta etapa, aplicamos a transformação logarítmica na variável alvo (`SalePrice`) para normalizar sua distribuição. Em seguida, concatenamos os dados de treino e teste para garantir consistência no tratamento de valores ausentes.\n",
    "\n",
    "A estratégia de imputação adotada baseia-se na natureza de cada variável:\n",
    "- **Ausência Física:** Valores nulos em variáveis como piscina ou garagem são preenchidos com \"None\" (categóricas) ou 0 (numéricas).\n",
    "- **Contexto Espacial:** `LotFrontage` é preenchida com a mediana da vizinhança.\n",
    "- **Dados Faltantes Gerais:** Utilização da moda para as demais variáveis categóricas.\n",
    "- **Limpeza:** Remoção da coluna `Utilities` por apresentar variância quase nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação Logarítmica na Variável Alvo\n",
    "train_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n",
    "print(\"Log-transformation aplicada em SalePrice.\")\n",
    "\n",
    "# Concatenação de Treino e Teste\n",
    "ntrain = train_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "y_train = train_df.SalePrice.values\n",
    "\n",
    "# Remove Id e SalePrice para criar o dataset 'all_data'\n",
    "all_data = pd.concat((train_df.drop(['Id', 'SalePrice'], axis=1), \n",
    "                      test_df.drop(['Id'], axis=1))).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dimensão de all_data antes da limpeza: {all_data.shape}\")\n",
    "\n",
    "# Imputação de Valores Ausentes\n",
    "\n",
    "# NA significa \"Ausência\" (Categoria 'None')\n",
    "cols_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', \n",
    "             'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "             'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "             'MasVnrType']\n",
    "\n",
    "for col in cols_none:\n",
    "    all_data[col] = all_data[col].fillna(\"None\")\n",
    "\n",
    "# NA significa \"Ausência\" (Valor 0 para numéricos)\n",
    "cols_zero = ['GarageYrBlt', 'GarageArea', 'GarageCars',\n",
    "             'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n",
    "             'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
    "\n",
    "for col in cols_zero:\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "\n",
    "# Preencher com a mediana da vizinhança\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Imputação pela moda\n",
    "cols_mode = ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', \n",
    "             'Exterior2nd', 'SaleType']\n",
    "\n",
    "for col in cols_mode:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "# Casos Específicos\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\") # Assumido Típico\n",
    "all_data = all_data.drop(['Utilities'], axis=1) # Variável quase constante\n",
    "\n",
    "# Verificação Final\n",
    "total_null = all_data.isnull().sum().sum()\n",
    "print(f\"Total de valores nulos restantes: {total_null}\")\n",
    "print(f\"Dimensão de all_data após limpeza: {all_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste de Tipos de Dados\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "\n",
    "# Label Encoding para Variáveis Ordinais\n",
    "cols_ordinal = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "                'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', \n",
    "                'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', \n",
    "                'GarageFinish', 'LandSlope', 'LotShape', 'PavedDrive', 'Street', \n",
    "                'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold')\n",
    "\n",
    "print(\"Aplicando Label Encoding...\")\n",
    "for c in cols_ordinal:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "\n",
    "# Criação de Novas Features\n",
    "# A área total da casa\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "print(f\"Variável 'TotalSF' criada.\")\n",
    "\n",
    "# Transformação de Variáveis Numéricas Assimétricas\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Calcula a assimetria de todas as variáveis numéricas\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "# Filtra aquelas com assimetria alta (> 0.75)\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(f\"Total de variáveis numéricas transformadas (Box-Cox): {skewness.shape[0]}\")\n",
    "\n",
    "# Suaviza a distribuição\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "\n",
    "# Transforma as variáveis categóricas nominais restantes em colunas binárias\n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(f\"Dimensão final do dataset: {all_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4952f",
   "metadata": {},
   "source": [
    "### Engenharia de Atributos e Transformação de Variáveis\n",
    "\n",
    "Nesta etapa, realizamos modificações cruciais nas features para adequá-las aos modelos de regressão:\n",
    "\n",
    "1.  **Ajuste de Tipos:** Conversão de variáveis numéricas que funcionam como categorias (como Ano e Mês de venda) para string.\n",
    "2.  **Label Encoding:** Transformação de variáveis categóricas ordinais em números, preservando a hierarquia de valor (ex: qualidade dos materiais).\n",
    "3.  **Criação de Features:** Geração da variável `TotalSF` (Área Total), somando as áreas do porão e dos andares habitáveis.\n",
    "4.  **Tratamento de Assimetria:** Aplicação da transformação de Box-Cox em variáveis numéricas com alta assimetria (> 0.75) para aproximá-las de uma distribuição normal.\n",
    "5.  **Dummy Variables:** Conversão final das variáveis categóricas nominais restantes em colunas binárias (One-Hot Encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39210dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação Final dos Dados\n",
    "# Garante que ntrain esteja sincronizado com o y_train\n",
    "ntrain = len(y_train)\n",
    "\n",
    "# Separa o dataset 'all_data' de volta em treino e teste\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "\n",
    "print(f\"Dimensões confirmadas -> Treino: {train.shape}, Target: {y_train.shape}\")\n",
    "\n",
    "# Configuração da Validação Cruzada\n",
    "# K-Fold com 5 divisões\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    # A métrica de erro negativo é convertida para RMSE positivo\n",
    "    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, \n",
    "                                    scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return(rmse)\n",
    "\n",
    "# Definição dos Modelos\n",
    "# Lasso para descartar features inúteis\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
    "\n",
    "# ElasticNet\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "# Kernel Ridge\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "# Gradient Boosting\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state=5)\n",
    "\n",
    "# Execução da Validação\n",
    "models = {'Lasso': lasso, 'ElasticNet': ENet, 'KernelRidge': KRR, 'GradientBoosting': GBoost}\n",
    "\n",
    "print(\"\\n--- Performance dos Modelos (RMSE Log - Quanto menor, melhor) ---\")\n",
    "for name, model in models.items():\n",
    "    score = rmsle_cv(model)\n",
    "    print(f\"{name}: {score.mean():.4f} (Desvio: {score.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b36d4",
   "metadata": {},
   "source": [
    "### Treinamento Final e Geração da Submissão\n",
    "\n",
    "Com base nos resultados da validação cruzada, o modelo Lasso foi selecionado por apresentar o melhor desempenho (menor RMSE) e simplicidade.\n",
    "\n",
    "Nesta etapa final, realizamos os seguintes procedimentos:\n",
    "1.  **Treinamento:** O pipeline do modelo é ajustado utilizando todo o conjunto de dados de treino disponível.\n",
    "2.  **Previsão:** Inferência dos preços sobre o conjunto de teste.\n",
    "3.  **Reversão da Escala:** Aplicação da função exponencial (`np.expm1`) para reverter a transformação logarítmica realizada no início do projeto, retornando os valores para a escala monetária original.\n",
    "4.  **Exportação:** Consolidação dos resultados no arquivo `submission.csv`, contendo os identificadores e as previsões de preço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4327a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição e Treinamento do Modelo Final\n",
    "# Lasso, pois apresentou a melhor performance na validação\n",
    "final_model = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
    "\n",
    "print(\"Treinando o modelo final em todos os dados de treino...\")\n",
    "final_model.fit(train, y_train)\n",
    "\n",
    "# Previsão nos Dados de Teste\n",
    "print(\"Realizando previsões no dataset de teste...\")\n",
    "predictions_log = final_model.predict(test)\n",
    "\n",
    "# Reversão da Escala Logarítmica\n",
    "predictions_real = np.expm1(predictions_log)\n",
    "\n",
    "# Geração do DataFrame de Submissão\n",
    "test_df_original = pd.read_csv('data/test.csv') # Lendo apenas para garantir os IDs corretos\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = test_df_original['Id']\n",
    "submission['SalePrice'] = predictions_real\n",
    "\n",
    "# Salvando o Arquivo CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Arquivo 'submission.csv' gerado com sucesso!\")\n",
    "\n",
    "# Visualizando as primeiras linhas\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
