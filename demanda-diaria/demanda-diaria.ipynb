{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccff3228",
   "metadata": {},
   "source": [
    "# Previsão de Demanda - VAI Store\n",
    "\n",
    "Este projeto tem como objetivo o desenvolvimento de um modelo de previsão de demanda diária para a rede de varejo \"VAI Store\". A variável alvo é a demanda para o mês de Dezembro de 2024.\n",
    "\n",
    "### Abordagem Técnica\n",
    "O problema foi modelado como uma Regressão de Séries Temporais utilizando o algoritmo XGBoost. Dada a natureza sequencial dos dados, foi adotada uma estratégia de Previsão Recursiva: a previsão de um dia é utilizada como *input* (lag) para a estimativa do dia seguinte, mitigando a ausência de dados futuros.\n",
    "\n",
    "### Pipeline de Execução\n",
    "1.  **Setup e Merge:** Unificação dos dados de vendas (treino/teste) com o cadastro de produtos.\n",
    "2.  **Feature Engineering:** Criação de variáveis de calendário, identificadores de feriados (Black Friday/Natal) e histórico de vendas (*Lags* e Médias Móveis).\n",
    "3.  **Análise Exploratória (EDA):** Verificação de sazonalidade semanal e correlações temporais.\n",
    "4.  **Modelagem e Previsão:** Treinamento com o histórico completo (Jan-Nov) e geração das previsões recursivas para Dezembro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9499e",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "As bibliotecas fundamentais para o projeto são importadas nesta etapa. Além dos pacotes padrão de manipulação de dados e visualização, o XGBoost é carregado para a modelagem preditiva, juntamente com as ferramentas do Scikit-Learn para o tratamento de variáveis categóricas.\n",
    "\n",
    "Também são ajustadas as configurações de exibição do Pandas para garantir a visualização completa das colunas, e os avisos de versão são silenciados para manter o notebook limpo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f46b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775cb3a",
   "metadata": {},
   "source": [
    "## 2. Carga e Tratamento Inicial dos Dados\n",
    "\n",
    "Os arquivos brutos foram carregados e preparados para a modelagem. Inicialmente, os metadados de produtos foram higienizados e a chave `SKU` foi padronizada para o formato numérico, prevenindo inconsistências durante a junção das tabelas.\n",
    "\n",
    "Na sequência, o histórico de vendas foi enriquecido com as informações de categoria e unidade através do *merge* com a tabela de produtos. Por fim, a coluna de data foi convertida para o formato apropriado e os registros foram ordenados cronologicamente, etapa essencial para a construção correta das variáveis de defasagem (*lags*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0745d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_produto = pd.read_csv('data/produto.csv')\n",
    "\n",
    "# Padroniza nomes de colunas e remove espaços em branco/aspas\n",
    "df_produto.columns = df_produto.columns.str.strip().str.lower().str.replace('\"', '')\n",
    "\n",
    "# Converte tudo para numérico para evitar erros de merge\n",
    "for df in [df_train, df_test, df_produto]:\n",
    "    if 'sku' in df.columns:\n",
    "        df['sku'] = pd.to_numeric(df['sku'], errors='coerce')\n",
    "\n",
    "# Trazemos categoria, subcategoria e unidade para as tabelas de vendas\n",
    "df_train = df_train.merge(df_produto, on='sku', how='left')\n",
    "df_test = df_test.merge(df_produto, on='sku', how='left')\n",
    "\n",
    "# Conversão de Datas e Ordenação\n",
    "df_train['data'] = pd.to_datetime(df_train['data'])\n",
    "df_test['data'] = pd.to_datetime(df_test['data'])\n",
    "\n",
    "# Ordenação para cálculos de Lag\n",
    "df_train = df_train.sort_values('data')\n",
    "df_test = df_test.sort_values('data')\n",
    "\n",
    "print(f\"Treino: {df_train.shape}, Teste: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a190f2",
   "metadata": {},
   "source": [
    "## 3. Engenharia de Features e Variáveis Temporais\n",
    "\n",
    "Uma função dedicada foi desenvolvida para enriquecer o dataset com variáveis explicativas fundamentais para o modelo. Extraem-se componentes de calendário (dia da semana, mês) e criam-se indicadores binários (*flags*) para eventos de pico, especificamente a Black Friday e a semana do Natal, permitindo que o modelo capture esses comportamentos atípicos.\n",
    "\n",
    "Adicionalmente, geram-se as variáveis de histórico (*lags*) e médias móveis agrupadas por SKU e Filial. Essa abordagem preserva a individualidade de cada série temporal e fornece ao algoritmo o contexto de vendas recentes (`lag_1`), sazonalidade semanal (`lag_7`) e tendência de curto prazo (`rolling_mean_7`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df855909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # Calendário Básico\n",
    "    df['dia_semana'] = df['data'].dt.dayofweek\n",
    "    df['dia_mes'] = df['data'].dt.day\n",
    "    df['mes'] = df['data'].dt.month\n",
    "    df['fim_de_semana'] = (df['dia_semana'] >= 5).astype(int)\n",
    "    \n",
    "    # Lags e Médias\n",
    "    groupby_cols = ['sku', 'cod_filial']\n",
    "    \n",
    "    # Lag 1: Venda de ontem\n",
    "    df['lag_1'] = df.groupby(groupby_cols)['demanda'].shift(1)\n",
    "    \n",
    "    # Lag 7: Venda de uma semana atrás\n",
    "    df['lag_7'] = df.groupby(groupby_cols)['demanda'].shift(7)\n",
    "    \n",
    "    # Média Móvel de 7 dias\n",
    "    df['rolling_mean_7'] = df.groupby(groupby_cols)['demanda'].transform(\n",
    "        lambda x: x.rolling(7, min_periods=1).mean().shift(1)\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2771b",
   "metadata": {},
   "source": [
    "## 4. Preparação do Dataset Completo e Codificação\n",
    "\n",
    "Os conjuntos de treino e teste foram concatenados em um único *dataframe* para garantir o tratamento uniforme dos dados. Foi aplicado um *reset* nos índices para assegurar a unicidade de cada registro, etapa crucial para evitar conflitos de duplicidade durante o *loop* de previsão recursiva.\n",
    "\n",
    "Na sequência, as variáveis categóricas foram transformadas em valores numéricos utilizando *Label Encoding*, adequando os dados aos requisitos do algoritmo XGBoost. Por fim, a função de engenharia de *features* foi aplicada a toda a base unificada, gerando os históricos (*lags*) iniciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e177284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenação\n",
    "df_train['set'] = 'train'\n",
    "df_test['set'] = 'test'\n",
    "df_test['demanda'] = np.nan # Placeholder\n",
    "\n",
    "# Adicionamos ignore_index=True para criar um índice novo e único de 0 a N\n",
    "df_all = pd.concat([df_train, df_test], sort=False, ignore_index=True)\n",
    "df_all = df_all.sort_values(['sku', 'cod_filial', 'data'])\n",
    "# Resetamos novamente após ordenar para garantir sequencialidade limpa\n",
    "df_all = df_all.reset_index(drop=True) \n",
    "\n",
    "# Label Encoding\n",
    "cat_cols = ['filial', 'unidade', 'categoria', 'subcategoria']\n",
    "encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_all[col] = df_all[col].astype(str)\n",
    "    df_all[col] = le.fit_transform(df_all[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# Aplicação Inicial de Features\n",
    "df_all_features = create_features(df_all)\n",
    "\n",
    "# Definição das variáveis\n",
    "features = ['sku', 'cod_filial', 'filial', 'unidade', 'categoria', 'subcategoria', \n",
    "            'dia_semana', 'dia_mes', 'mes', 'fim_de_semana',\n",
    "            'lag_1', 'lag_7', 'rolling_mean_7']\n",
    "target = 'demanda'\n",
    "\n",
    "print(\"Features e índices finalizados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec108156",
   "metadata": {},
   "source": [
    "## 4.1. Análise Exploratória: Visão Temporal e Distribuição\n",
    "\n",
    "Nesta etapa, investiga-se o comportamento macroscópico da série temporal. Gera-se um gráfico de linha da demanda total diária para identificar tendências, sazonalidades e picos associados a datas comemorativas.\n",
    "\n",
    "Simultaneamente, examina-se a distribuição da variável alvo através de um histograma, o que permite avaliar a assimetria dos dados e a concentração de volumes. Por fim, utiliza-se um *boxplot* para comparar a dispersão e a mediana de vendas entre as principais categorias de produtos, evidenciando as diferenças de comportamento entre os grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de Estilo\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Tendência geral e picos\n",
    "plt.subplot(2, 1, 1)\n",
    "daily_sales = df_all_features[df_all_features['set'] == 'train'].groupby('data')['demanda'].sum()\n",
    "sns.lineplot(x=daily_sales.index, y=daily_sales.values, color='royalblue', linewidth=2)\n",
    "plt.title('Evolução da Demanda Total Diária (Jan - Out)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Vendas Totais (Unidades/Kg)')\n",
    "plt.xlabel('Data')\n",
    "\n",
    "# Adicionando destaque para datas de pico potenciais\n",
    "plt.axvline(pd.Timestamp('2024-05-12'), color='r', linestyle='--', alpha=0.5, label='Dia das Mães') # Exemplo\n",
    "plt.legend()\n",
    "\n",
    "# Para ver se a demanda segue uma normal ou é muito assimetrica\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df_all_features[df_all_features['set'] == 'train']['demanda'], bins=50, kde=True, color='teal')\n",
    "plt.title('Distribuição da Demanda (Histograma)', fontsize=12)\n",
    "plt.xlabel('Demanda')\n",
    "\n",
    "plt.xlim(0, 50) \n",
    "\n",
    "# Boxplot por Categoria\n",
    "plt.subplot(2, 2, 4)\n",
    "top_cats = df_all_features[df_all_features['set'] == 'train']['categoria'].value_counts().head(5).index\n",
    "sns.boxplot(x='categoria', y='demanda', data=df_all_features[df_all_features['categoria'].isin(top_cats)], showfliers=False, palette='viridis')\n",
    "plt.title('Dispersão de Vendas por Categoria (Top 5)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9825af",
   "metadata": {},
   "source": [
    "## 4.2. Validação de Hipóteses: Sazonalidade e Autocorrelação\n",
    "\n",
    "Nesta seção, aprofunda-se a análise estatística das vendas. Inicialmente, avalia-se a sazonalidade semanal através da média de vendas por dia da semana, buscando identificar padrões de comportamento específicos de dias úteis versus finais de semana.\n",
    "\n",
    "Na sequência, verifica-se a força da memória temporal da série através de gráficos de dispersão. Analisa-se a correlação linear entre a demanda atual e as vendas do dia anterior (`lag_1`), bem como a relação com as vendas de uma semana atrás (`lag_7`), validando a premissa de que o histórico recente e os ciclos semanais são fortes preditores para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ae11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Sazonalidade Semanal\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='dia_semana', y='demanda', data=df_all_features[df_all_features['set'] == 'train'], palette='Blues_d', ci=None)\n",
    "plt.title('Média de Vendas por Dia da Semana\\n(0=Seg, 6=Dom)', fontsize=12)\n",
    "plt.ylabel('Média de Demanda')\n",
    "plt.xlabel('Dia da Semana')\n",
    "\n",
    "# Correlação Lag 1\n",
    "plt.subplot(1, 3, 2)\n",
    "\n",
    "sample_df = df_all_features[df_all_features['set'] == 'train'].sample(10000, random_state=42)\n",
    "sns.scatterplot(x='lag_1', y='demanda', data=sample_df, alpha=0.3, color='purple')\n",
    "plt.title(f\"Correlação Demanda x Venda Ontem (Lag 1)\\nCorr: {sample_df['demanda'].corr(sample_df['lag_1']):.2f}\", fontsize=12)\n",
    "plt.xlabel('Vendas Ontem')\n",
    "plt.ylabel('Vendas Hoje')\n",
    "\n",
    "plt.plot([0, 100], [0, 100], 'r--') \n",
    "plt.xlim(0, 100); plt.ylim(0, 100)\n",
    "\n",
    "# 3. Correlação Lag 7\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(x='lag_7', y='demanda', data=sample_df, alpha=0.3, color='darkorange')\n",
    "plt.title(f\"Correlação Demanda x Semana Passada (Lag 7)\\nCorr: {sample_df['demanda'].corr(sample_df['lag_7']):.2f}\", fontsize=12)\n",
    "plt.xlabel('Vendas 7 Dias Atrás')\n",
    "plt.ylabel('Vendas Hoje')\n",
    "plt.plot([0, 100], [0, 100], 'r--')\n",
    "plt.xlim(0, 100); plt.ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878ec3b",
   "metadata": {},
   "source": [
    "## 5. Treinamento do Modelo\n",
    "\n",
    "Nesta etapa, o conjunto de treino é isolado do *dataset* unificado, e separam-se as variáveis preditoras ($X$) da variável alvo ($y$).\n",
    "\n",
    "O modelo XGBoost é configurado com hiperparâmetros ajustados para o refinamento do aprendizado: definiu-se uma taxa de aprendizado conservadora (`0.03`) combinada com uma maior profundidade de árvores (`7`), permitindo a captura de padrões não lineares complexos. O treinamento é então executado sobre a totalidade dos dados históricos disponíveis (Janeiro a Novembro), maximizando a exposição do algoritmo às tendências recentes e aos eventos de pico antes do período de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd25ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação dos dados\n",
    "train_mask = df_all_features['set'] == 'train'\n",
    "X_train = df_all_features[train_mask][features]\n",
    "y_train = df_all_features[train_mask][target]\n",
    "\n",
    "# Pesos Temporais\n",
    "dates_train = df_all_features[train_mask]['data']\n",
    "min_date = dates_train.min()\n",
    "days_from_start = (dates_train - min_date).dt.days\n",
    "\n",
    "sample_weights = (days_from_start / days_from_start.max()) ** 2\n",
    "\n",
    "# Configuração e Treino\n",
    "print(\"Iniciando treinamento com Ponderação Temporal (Foco em Recência)...\")\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Passamos os pesos no fit\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)\n",
    "\n",
    "print(\"Modelo treinado! (Novembro teve muito mais peso que Janeiro)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1dc2ea",
   "metadata": {},
   "source": [
    "## 6. Execução da Previsão Recursiva\n",
    "\n",
    "Implementa-se aqui a estratégia central para mitigar a ausência de dados futuros. Um *loop* iterativo percorre cada dia do período de teste sequencialmente. Em cada iteração, as *features* temporais são recalculadas dinamicamente, permitindo que a previsão do dia anterior ($t-1$) preencha o *lag* necessário para a estimativa do dia atual ($t$).\n",
    "\n",
    "As predições geradas são imediatamente integradas ao *dataset* histórico (`df_recursive`), alimentando as variáveis de defasagem dos dias subsequentes. Adicionalmente, aplica-se uma restrição para garantir que não sejam gerados valores de demanda negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando previsão recursiva (Escala Real)...\")\n",
    "\n",
    "test_dates = df_all[df_all['set'] == 'test']['data'].sort_values().unique()\n",
    "df_recursive = df_all.copy()\n",
    "\n",
    "for current_date in test_dates:\n",
    "    current_date_ts = pd.Timestamp(current_date)\n",
    "    \n",
    "    # Recalcula features\n",
    "    df_feat_step = create_features(df_recursive)\n",
    "    \n",
    "    # Filtra dia atual\n",
    "    mask_today = df_feat_step['data'] == current_date_ts\n",
    "    X_step = df_feat_step.loc[mask_today, features]\n",
    "    \n",
    "    if len(X_step) > 0:\n",
    "        preds = model.predict(X_step)\n",
    "        \n",
    "        preds = np.maximum(preds, 0)\n",
    "        \n",
    "        # Salva para o próximo passo\n",
    "        df_recursive.loc[mask_today, 'demanda'] = preds\n",
    "\n",
    "print(\"Aplicando multiplicador de Sazonalidade...\")\n",
    "mask_test = df_recursive['set'] == 'test'\n",
    "\n",
    "# Natal : Aumento de 10%\n",
    "natal_mask = mask_test & (df_recursive['data'].dt.day >= 19) & (df_recursive['data'].dt.day <= 24)\n",
    "df_recursive.loc[natal_mask, 'demanda'] = df_recursive.loc[natal_mask, 'demanda'] * 1.10\n",
    "\n",
    "# Ano Novo: Aumento de 5%\n",
    "ano_novo_mask = mask_test & (df_recursive['data'].dt.day >= 29) & (df_recursive['data'].dt.day <= 31)\n",
    "df_recursive.loc[ano_novo_mask, 'demanda'] = df_recursive.loc[ano_novo_mask, 'demanda'] * 1.05\n",
    "\n",
    "print(\"Processo finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9047050",
   "metadata": {},
   "source": [
    "## 7. Consolidação e Geração da Submissão Final\n",
    "\n",
    "Nesta etapa conclusiva, isolam-se as previsões geradas para o conjunto de teste. Para garantir a integridade da estrutura exigida pela competição, realiza-se um *merge* entre as previsões calculadas e o arquivo de submissão base (`test.csv`), utilizando as chaves compostas (`data`, `sku`, `cod_filial`) como referência.\n",
    "\n",
    "Este procedimento assegura que cada `id` original receba sua respectiva demanda prevista, mantendo a ordenação correta. Por fim, o arquivo CSV é exportado contendo apenas as colunas `id` e `demanda`, pronto para ser submetido à plataforma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas o set de teste já preenchido\n",
    "df_final = df_recursive[df_recursive['set'] == 'test'].copy()\n",
    "\n",
    "# Merge com o arquivo de submissão original\n",
    "submission_base = pd.read_csv('data/test.csv')\n",
    "submission_base['data'] = pd.to_datetime(submission_base['data'])\n",
    "\n",
    "# Selecionamos apenas as colunas chave e a demanda prevista\n",
    "df_final_clean = df_final[['data', 'sku', 'cod_filial', 'demanda']]\n",
    "\n",
    "# Merge\n",
    "submission = submission_base.merge(df_final_clean, on=['data', 'sku', 'cod_filial'], how='left')\n",
    "\n",
    "# Exportação\n",
    "filename = 'submission.csv'\n",
    "final_sub = submission[['id', 'demanda']]\n",
    "final_sub.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Arquivo '{filename}' gerado com {len(final_sub)} linhas.\")\n",
    "print(final_sub.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
